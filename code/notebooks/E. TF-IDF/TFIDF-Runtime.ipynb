{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e69c2-b065-46fa-b6eb-7ea56ae43f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow import keras \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1014d33e-c803-41ce-be12-ab1c3f929253",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),'python-packages/'))\n",
    "from deep_hashing_models import *\n",
    "from similarities import *\n",
    "from lsh_search import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c738c-cd1f-4e1b-ae93-1aa3d25882aa",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96659a4-eea0-48d9-8977-fecba559b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_repo = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))),'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2874701-b390-405b-9001-822faf31ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_repo + 'stack_traces.csv', index_col = [0])\n",
    "df['stackTraceCusto'] = df['stackTraceCusto'].apply(lambda x : x.replace('\\r',''))\n",
    "df['stackTraceCusto'] = df['stackTraceCusto'].apply(lambda x : x.replace('\\na','\\n'))\n",
    "df['listStackTrace'] = df['stackTraceCusto'].apply(lambda x : x.replace('\\n', ' ').strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd508b5-f286-4269-9c8d-0c2fba8db224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_stacks = pd.read_csv(data_repo + 'frequent_stack_traces.csv', index_col = [0])\n",
    "df_distinct_stacks['listStackTrace'] = df_distinct_stacks['stackTraceCusto'].apply(lambda x : x.replace('\\n', ' ').strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4022df1-8256-4289-b53a-506f5b1896b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_measures = pd.read_csv(data_repo + 'similarity-measures-pairs.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4772e1-6603-4344-a277-70d9728c61ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_stacks = df_distinct_stacks.shape[0]\n",
    "n_stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f7b639-45f5-48cf-80f1-769c0fc45e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bag_of_frames = CountVectorizer(token_pattern = r\"(?u)\\b[a-zA-Z0-9_.]{2,}\\b\")\n",
    "s = df_distinct_stacks['stackTraceCusto'].apply(lambda x : x.replace('\\n',' '))\n",
    "s = s.apply(lambda x : x.replace('$',''))\n",
    "s = s.apply(lambda x : x.replace('/',''))\n",
    "s = s.apply(lambda x : x.replace('<',''))\n",
    "s = s.apply(lambda x : x.replace('>',''))\n",
    "X_bag_of_frames = vectorizer_bag_of_frames.fit_transform(list(s)).toarray()\n",
    "df_bag_of_frames = pd.DataFrame(data = X_bag_of_frames, columns = vectorizer_bag_of_frames.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee3c8f08-8d38-4734-ab38-abbc7610353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2249)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit = 100000\n",
    "data_test = vectorizer_bag_of_frames.transform(df['stackTraceCusto'][:limit])\n",
    "data_test_df = pd.DataFrame.sparse.from_spmatrix(data_test)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acbe0bc5-a817-4209-83e0-43d22b77e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tf_idf = TfidfVectorizer(token_pattern = r\"(?u)\\b[a-zA-Z0-9_.]{2,}\\b\")\n",
    "s = df_distinct_stacks['stackTraceCusto'].apply(lambda x : x.replace('\\n',' '))\n",
    "s = s.apply(lambda x : x.replace('$',''))\n",
    "s = s.apply(lambda x : x.replace('/',''))\n",
    "s = s.apply(lambda x : x.replace('<',''))\n",
    "s = s.apply(lambda x : x.replace('>',''))\n",
    "X_tf_idf = vectorizer_tf_idf.fit_transform(list(s)).toarray()\n",
    "df_tf_idf = pd.DataFrame(data = X_tf_idf, columns = vectorizer_tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab6d1064-c66e-4cee-8d48-9f6478a632ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2249)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_tfidf = vectorizer_tf_idf.transform(df['stackTraceCusto'][:limit])\n",
    "data_test_tfidf_df = pd.DataFrame.sparse.from_spmatrix(data_test_tfidf)\n",
    "data_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa15e5-aee6-46ea-9c52-d1fa077b05dc",
   "metadata": {},
   "source": [
    "# 2. Load deeplsh and baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cc1ca1-6bf7-4e79-89f9-6c16af9db70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "intermediate_model_deeplsh  = keras.models.load_model('Models/model-deep-lsh.model')\n",
    "intermediate_model_baseline = keras.models.load_model('Models/model-baseline.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f56e1-4f1b-4b03-97a2-0d77a14926cc",
   "metadata": {},
   "source": [
    "# 3. Runtime comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a03889-5b2e-47b4-a84e-db1f7740946a",
   "metadata": {},
   "source": [
    "## 3.1. Brute force method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1d384f5-2faf-4e4d-8829-35d232584e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 21min 50s, sys: 6.15 s, total: 2h 21min 56s\n",
      "Wall time: 2h 22min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sim_tfidf = data_test_df[:limit].apply(lambda x : cosine_similarity_df(x, df_tf_idf, rowIndex(x)), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41b1cc-50dc-4ea5-ad36-a7e41bef5319",
   "metadata": {},
   "source": [
    "## 3.2. DeepLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "418616de-97c2-4f55-b8bb-7b238e73e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Hash-Tables/hash_tables_deeplsh.pkl', 'rb') as f:\n",
    "    hash_tables_deeplsh = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154bdb34-c1ae-48e4-bc51-f980a5de8c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 1.57 s, total: 16.9 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction_deeplsh = intermediate_model_deeplsh.predict(data_test_tfidf_df.values)\n",
    "hash_vectors_deeplsh = convert_to_hamming(prediction_deeplsh)\n",
    "_ = pd.Series(np.arange(limit)).apply(lambda x : near_duplicates_for_runtime(8, 8, 8, x, hash_vectors_deeplsh, hash_tables_deeplsh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec26f5-2445-420d-94bd-0fdab0716a48",
   "metadata": {},
   "source": [
    "## 3.3. baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee4ccaf-2f6c-465a-ba01-b17a46c0aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Hash-Tables/hash_tables_baseline.pkl', 'rb') as f:\n",
    "    hash_tables_baseline = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90303bc4-bbc4-46b8-998f-44320478b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 1.3 s, total: 17.7 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction_baseline = intermediate_model_baseline.predict(data_test_tfidf_df.values)\n",
    "hash_vectors_baseline = convert_to_hamming(prediction_baseline)\n",
    "_ = pd.Series(np.arange(limit)).apply(lambda x : near_duplicates_for_runtime(8, 8, 8, x, hash_vectors_baseline, hash_tables_baseline))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "split-sd4x",
   "language": "python",
   "name": "split-sd4x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
