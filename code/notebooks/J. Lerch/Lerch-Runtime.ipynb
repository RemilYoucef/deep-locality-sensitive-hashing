{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e69c2-b065-46fa-b6eb-7ea56ae43f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow import keras \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1014d33e-c803-41ce-be12-ab1c3f929253",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),'python-packages/'))\n",
    "from deep_hashing_models import *\n",
    "from similarities import *\n",
    "from lsh_search import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c738c-cd1f-4e1b-ae93-1aa3d25882aa",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96659a4-eea0-48d9-8977-fecba559b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_repo = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))),'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2874701-b390-405b-9001-822faf31ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_repo + 'stack_traces.csv', index_col = [0])\n",
    "df['stackTraceCusto'] = df['stackTraceCusto'].apply(lambda x : x.replace('\\r',''))\n",
    "df['stackTraceCusto'] = df['stackTraceCusto'].apply(lambda x : x.replace('\\na','\\n'))\n",
    "df['listStackTrace'] = df['stackTraceCusto'].apply(lambda x : x.replace('\\n', ' ').strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd508b5-f286-4269-9c8d-0c2fba8db224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_stacks = pd.read_csv(data_repo + 'frequent_stack_traces.csv', index_col = [0])\n",
    "df_distinct_stacks['listStackTrace'] = df_distinct_stacks['stackTraceCusto'].apply(lambda x : x.replace('\\n', ' ').strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4022df1-8256-4289-b53a-506f5b1896b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_measures = pd.read_csv(data_repo + 'similarity-measures-pairs.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4772e1-6603-4344-a277-70d9728c61ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_stacks = df_distinct_stacks.shape[0]\n",
    "n_stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbe0bc5-a817-4209-83e0-43d22b77e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bag_of_frames = CountVectorizer(token_pattern = r\"(?u)\\b[a-zA-Z0-9_.]{2,}\\b\")\n",
    "s = df_distinct_stacks['stackTraceCusto'].apply(lambda x : x.replace('\\n',' '))\n",
    "s = s.apply(lambda x : x.replace('$',''))\n",
    "s = s.apply(lambda x : x.replace('/',''))\n",
    "s = s.apply(lambda x : x.replace('<',''))\n",
    "s = s.apply(lambda x : x.replace('>',''))\n",
    "X_bag_of_frames = vectorizer_bag_of_frames.fit_transform(list(s)).toarray()\n",
    "df_bag_of_frames = pd.DataFrame(data = X_bag_of_frames, columns = vectorizer_bag_of_frames.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3c8f08-8d38-4734-ab38-abbc7610353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2249)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit = 100000\n",
    "data_test = vectorizer_bag_of_frames.transform(df['stackTraceCusto'][:limit])\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89df1703-3fea-4167-bd04-d3fa215c5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_idf_frames = df_bag_of_frames.sum(axis = 0).apply(lambda x : 1 + math.log(df_bag_of_frames.shape[0] / x)).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa15e5-aee6-46ea-9c52-d1fa077b05dc",
   "metadata": {},
   "source": [
    "# 2. Load deeplsh and baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9cc1ca1-6bf7-4e79-89f9-6c16af9db70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "intermediate_model_deeplsh  = keras.models.load_model('Models/model-deep-lsh.model')\n",
    "intermediate_model_baseline = keras.models.load_model('Models/model-baseline.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f56e1-4f1b-4b03-97a2-0d77a14926cc",
   "metadata": {},
   "source": [
    "# 3. Runtime comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a03889-5b2e-47b4-a84e-db1f7740946a",
   "metadata": {},
   "source": [
    "## 3.1. Brute force method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "876b8a03-a9b3-4646-bce1-bb7a98acdd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51min 58s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "result = %timeit -n1 -r1 -o df[:limit].apply(lambda x : lerch_df(x['listStackTrace'], df_distinct_stacks['listStackTrace'], rowIndex(x), dict_idf_frames), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2dc8514-95ab-49ba-b50b-49aa18bd265e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 51min 58s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41b1cc-50dc-4ea5-ad36-a7e41bef5319",
   "metadata": {},
   "source": [
    "## 3.2. DeepLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "418616de-97c2-4f55-b8bb-7b238e73e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Hash-Tables/hash_tables_deeplsh.pkl', 'rb') as f:\n",
    "    hash_tables_deeplsh = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ecff04a-54a8-4fd0-93cd-c51460c828ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 s, sys: 1.92 s, total: 25.5 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction_deeplsh = intermediate_model_deeplsh.predict(data_test)\n",
    "hash_vectors_deeplsh = convert_to_hamming(prediction_deeplsh)\n",
    "_ = pd.Series(np.arange(limit)).apply(lambda x : near_duplicates_for_runtime(4, 16, 16, x, hash_vectors_deeplsh, hash_tables_deeplsh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec26f5-2445-420d-94bd-0fdab0716a48",
   "metadata": {},
   "source": [
    "## 3.3. baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4ccaf-2f6c-465a-ba01-b17a46c0aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Hash-Tables/hash_tables_baseline.pkl', 'rb') as f:\n",
    "    hash_tables_baseline = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90303bc4-bbc4-46b8-998f-44320478b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction_baseline = intermediate_model_baseline.predict(data_test)\n",
    "hash_vectors_baseline = convert_to_hamming(prediction_baseline)\n",
    "_ = pd.Series(np.arange(limit)).apply(lambda x : near_duplicates_for_runtime(8, 8, 16, x, hash_vectors_baseline, hash_tables_baseline))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "split-sd4x",
   "language": "python",
   "name": "split-sd4x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
